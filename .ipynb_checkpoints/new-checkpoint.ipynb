{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before cleaning: 315200\n",
      "Number of rows after cleaning: 168842\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Load the first dataset\n",
    "df1 = pd.read_csv('badmintondata.csv')\n",
    "\n",
    "# Load the second dataset\n",
    "df2 = pd.read_csv('badmintondata2.csv')\n",
    "\n",
    "# Combine both datasets\n",
    "df_combined = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Count rows before cleaning\n",
    "rows_before_cleaning = df_combined.shape[0]\n",
    "\n",
    "# Handling missing values\n",
    "df_combined = df_combined.dropna()  # Drop rows with missing values\n",
    "\n",
    "# Handling outliers (Using z-score)\n",
    "z_scores_combined = (df_combined - df_combined.mean()) / df_combined.std()  # Calculate z-scores\n",
    "threshold = 3  # Set a threshold for outliers\n",
    "outliers_combined = (z_scores_combined.abs() > threshold).any(axis=1)  # Find rows with outliers\n",
    "df_combined_cleaned = df_combined[~outliers_combined].copy()  # Filter out rows with outliers\n",
    "\n",
    "# Identify consecutive rows of all zeros\n",
    "mask = (df_combined == 0).all(axis=1)\n",
    "groups = mask.cumsum()\n",
    "\n",
    "# Filter out consecutive rows of all zeros\n",
    "df_combined_cleaned = df_combined[~mask]\n",
    "df_combined_cleaned.to_csv('cleaned_dataset.csv', index = False)\n",
    "\n",
    "# Count rows after cleaning\n",
    "rows_after_cleaning = df_combined_cleaned.shape[0]\n",
    "\n",
    "print(\"Number of rows before cleaning:\", rows_before_cleaning)\n",
    "print(\"Number of rows after cleaning:\", rows_after_cleaning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data before each group of zeros\n",
    "grouped_df = df_combined_cleaned.groupby(groups, group_keys=False)\n",
    "\n",
    "# Function to add a Time field and SERVE_ID to a group\n",
    "def add_time_and_serve_id(group):\n",
    "    group['TIME'] = np.arange(len(group)) * 10  # Multiply by 10 to get time in ms\n",
    "    group['SERVE_ID'] = group.name  # Assign the group name as the SERVE_ID\n",
    "\n",
    "    # Reset time to 0 at the start of each group\n",
    "    group.loc[group.index[0], 'TIME'] = 0\n",
    "    \n",
    "    return group\n",
    "\n",
    "\n",
    "# Apply a TIME-based transformation and add SERVE_ID to each group\n",
    "grouped_df = grouped_df.apply(add_time_and_serve_id)\n",
    "\n",
    "# Extract unique groups\n",
    "serve_ids = grouped_df['SERVE_ID'].unique()\n",
    "\n",
    "# Split the groups into testing set and sample set (split into a testing set and a sample set using a 70:30 ratio)\n",
    "train_groups, test_groups = train_test_split(serve_ids, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create the training set\n",
    "train_df = pd.concat([grouped_df[grouped_df['SERVE_ID'] == group] for group in train_groups])\n",
    "\n",
    "# Create the testing set\n",
    "test_df = pd.concat([grouped_df[grouped_df['SERVE_ID'] == group] for group in test_groups])\n",
    "\n",
    "# Print two unique groups\n",
    "for group in serve_ids[:2]:\n",
    "    group_data = grouped_df[grouped_df['SERVE_ID'] == group]\n",
    "    print(f\"Group: {group}\")\n",
    "    print(group_data)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the non-feature columns\n",
    "non_feature_cols = [\"HUMAN PLAYER POSITION (X) metres\", \"HUMAN PLAYER POSITION (Y) metres\",\n",
    "                    \"INITITAL VELOCITY OF SHUTTELCOCK(m/s)\", \"INITIAL SHUTTELCOCK FIRING ANGLE (DEGREE)\",\n",
    "                    \"SHUTTELCOCK SLANT ANGLE TO SIDELINE(DEGREE)\", \"TIME\"]\n",
    "\n",
    "# Define the feature columns\n",
    "feature_cols = [\"SHUTTLECOCK POSITIION IN AIR(X) metres\", \"SHUTTLECOCK POSITIION IN AIR(Y) metres\",\n",
    "                \"SHUTTLECOCK POSITIION IN AIR(Z) metres\"]\n",
    "\n",
    "# Create the training feature DataFrame\n",
    "train_features = train_df[feature_cols].copy()\n",
    "\n",
    "# Create the training non-feature DataFrame\n",
    "train_non_features = train_df[non_feature_cols].copy()\n",
    "\n",
    "# Create the testing feature DataFrame\n",
    "test_features = test_df[feature_cols].copy()\n",
    "\n",
    "# Create the testing non-feature DataFrame\n",
    "test_non_features = test_df[non_feature_cols].copy()\n",
    "\n",
    "# Combine the feature and non-feature DataFrames for training and testing\n",
    "X_train = pd.concat([train_features, train_non_features], axis=1)\n",
    "X_test = pd.concat([test_features, test_non_features], axis=1)\n",
    "y_train = train_df[\"SHUTTLECOCK POSITIION IN AIR(Z) metres\"]\n",
    "y_test = test_df[\"SHUTTLECOCK POSITIION IN AIR(Z) metres\"]\n",
    "\n",
    "# Train the model\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "print(\"RMSE:\", rmse)\n",
    "\n",
    "# Evaluate other metrics or perform additional analysis if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
